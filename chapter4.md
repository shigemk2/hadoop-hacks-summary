# 4 Hive Hacks

- Hive Hadoopのビッグデータ活用を容易にするツール
    - MapReduceのコードを組み上げても良いが欠点がある
        - 初期のアドホック分析の手間
        - 人的リソースと教育コスト
            - MapReduceジョブが難しい
− そのためのHiveQL(SQLに似た命令文)

- 現場のノウハウ
- システムとの連携
- パフォーマンスの高いHiveQL

## 34 SQLとHiveQLの違い

RDBMSのSQL文をSQLライクなHiveQL文に変換するさいのポイント

- メリット MapReduceを意識することなくSQLライクに記述する
- デメリット MapReduceを意識せずに記述することで性能が著しく落ちてしまう

### ポイント

- データの更新ができない
    - 一部更新はできない
- MapReduceタスクのオーバーヘッドがつきまとう
    - 高スループットを目指す処理に向いているが低レイテンシー処理には向いていない
- 並列分散のできない処理はHive向きでない
    - レコード全体をソートする処理
- FROM句に書けるテーブルの数は1つ
- サブクエリを持つINメソッド句は使えない
- 動的クエリ(${env:変数名})みたいな文字列で動的なクエリが使える
- 逆結合(anti join)に対応していない
- 集計時における重複レコードが許されない
- select句に存在しないカラムでのソートはできない
- NULLとカラ文字列は区別される
- rownumの取得はできない

## 35 Hiveの実運用でケアすること

- 検索範囲の限定が性能に与える影響は顕著
- パーティションごとにinsert/deleteができる

- 囲み文字、区切り文字を指定できる
- プライマリキーが存在しない
- データロードのときにエラーチェックしない
- 複雑な文字列置換処理を行うときはUDFの検討を
- where句の<と>はオブジェクトに変換する =のときはByte列のまま
- order by/distinctでキー数が多すぎるとOOMになる
- JoinをつかうときはStreaming(ファイル上)/HashMap(メモリ上)かで選べる

## 36 クエリの高速化(基本)

HiveQLは実行時にMapReduce処理に変換されるので、MapReduceの特性を知る必要がある

- 処理単位
    - 変数を使ってselect文を複数回投げるループ処理を行うと、1回のselectで全readが発生してしまう
- order by/sort by
    - Hiveはソート処理を得意としない
- よく使うデータなら結合済みデータを予め作成しておく
- `select * from movie LIMIT 50;` のようにMapReduceタスクが発生しないクエリは高速
- 3つ以上のテーブルを同じキーで結合する場合、2回ジョインしないで1回でジョインしたほうが早い

- 処理内容によっては処理時間のほとんどMapReduceのオーバーヘッドに費やされる

## 37 クエリの高速化

1. Mapタスクのみで完結する
2. Shuffleフェーズに流れるデータ量を抑制する
3. 同じテーブルをスキャンする回数を減らす

従来RDBMSで処理していたクエリをほぼそのままHiveにマイグレーションしても性能がでない可能性

## 38 ユーザ定義関数

HiveはSQLと同じく関数を利用できる(abs/substr)し、独自に関数を定義できる

- UDF(user defined function)
    - Java
- UADF(user defined aggregation function)
    - UADFのサブクラス
- UDTF(user defined table function)

複雑なクエリの単純化や予め用意されている関数では不可能な処理を実現できる

## 39 他システムと連携する

データファイルの相互互換

- SerDe(シリアライズ/デシリアライズ)
- RegexSerDe
- CSV用非標準SerDe
- JSON用の非標準SerDe

## 40 独自処理を組み込み

- transform句
    - 特定テーブルの各行にたいしてフィルターのような処理をおこなう
    - さらにその結果をまたテーブルでるかのように処理にわたす
- UDAFをつかう
    - 1行を読んで1行出力するフィルタ vs 複数行を読んで1つ出力する集約型

- 標準入出力を別プロセスで処理
- Hiveのアドオン関数
